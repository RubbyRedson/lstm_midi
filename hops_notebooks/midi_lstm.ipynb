{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MIDI LSTM Music Generator\n",
    "This houses the code for training an LSTM neural network on a text-based intermediate representation and creating a model which can predict (or \"compose\") music based on the learned weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wrapper():\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from tensorflow.contrib import rnn\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    # Filesystem setup\n",
    "    from hops import hdfs\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    \n",
    "    # Parsing constants\n",
    "    PROJECT_DIR = hdfs.project_path()\n",
    "    SAMPLE_DIR = PROJECT_DIR + \"ParsedMIDI/Sample\"\n",
    "    CHORD_DELIM = ' '\n",
    "    \n",
    "    # TODO Make directory with /**/**.txt\n",
    "    input_file = SAMPLE_DIR + \"/beethoven_hammerklavier_1_format0_track[0].txt\"\n",
    "    save_loc = PROJECT_DIR + \"SavedModels/model.ckpt\"\n",
    "    \n",
    "    hdfs.log(\"input_file: {}\".format(input_file))\n",
    "    hdfs.log(\"save_loc: {}\".format(save_loc))\n",
    "    \n",
    "    \"\"\" HELPER FUNCTIONS \"\"\"\n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def build_dicts(words):\n",
    "        \"\"\" TODO Use below, but figure out how to handle distributed/parallel workers and multi-note chords\n",
    "        d = {}\n",
    "        rd = {}\n",
    "        for i in range(0, 127):\n",
    "            d[i] = chr(i)\n",
    "            rd[chr(i)] = i\n",
    "        return (d, rd)\n",
    "        \"\"\"\n",
    "        import collections\n",
    "        counts = collections.Counter(words).most_common()\n",
    "        dictionary = dict()\n",
    "        for word, _count in counts:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        return dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "    def read_file(handle, file_path):\n",
    "        \"\"\"\n",
    "        Consumes a given textfile by stripping and splitting it into words\n",
    "        :param handle: HDFS handle\n",
    "        :param file_path: location of file to read\n",
    "        :return: numpy array of the file's \"words\"\n",
    "        \"\"\"\n",
    "        with fs_handle.open_file(file_path, \"r\") as f:\n",
    "            lines = []\n",
    "            while True:\n",
    "                try:\n",
    "                    lines.append(f.next())\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        content = [lines[i].split(CHORD_DELIM) for i in range(len(lines))]\n",
    "        content = np.array(content)\n",
    "        content = np.reshape(content, [-1, ])\n",
    "        return content\n",
    "    \n",
    "    def read_data(handle, data_path):\n",
    "        # TODO Handle directories!\n",
    "        return [read_file(handle, data_path)]\n",
    "    \n",
    "    def get_lstm_cell(num_hidden):\n",
    "        \"\"\"\n",
    "        Creates a new LSTM cell\n",
    "        :param num_hidden: number of units in the cell\n",
    "        :return a new LSTM cell\n",
    "        \"\"\"\n",
    "        return rnn.BasicLSTMCell(num_hidden)\n",
    "    \n",
    "    def create_rnn(x, weights, biases, num_inputs, num_hidden):\n",
    "        \"\"\"\n",
    "        Creates a multi-layer RNN based on LSTM cells\n",
    "        :param x: inputs\n",
    "        :param weights: dictionary of weight variables\n",
    "        :param biases: dictionary of bias variables\n",
    "        :param num_inputs: # of sequence inputs to the RNN\n",
    "        :param num_hidden: # of units in the RNN cells\n",
    "        :return the fully constructed RNN\n",
    "        \"\"\"\n",
    "        \n",
    "        def make_cell():\n",
    "            \"\"\"\n",
    "            Generates an RNN cell\n",
    "            :return the fully constructed RNN cell with optional dropoff (TODO)\n",
    "            \"\"\"\n",
    "            cell = get_lstm_cell(num_hidden)\n",
    "            # TODO if is_training and keep_prob < 1:\n",
    "            #    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=config.keep_prob)\n",
    "            return cell\n",
    "\n",
    "        x = tf.reshape(x, [-1, num_inputs])\n",
    "\n",
    "        # Split into n-element sequences of inputs\n",
    "        x = tf.split(x, num_inputs, 1)\n",
    "\n",
    "        num_layers = 2\n",
    "\n",
    "        rnn_cell = rnn.MultiRNNCell([make_cell() for _ in range(num_layers)])\n",
    "\n",
    "        # Generate prediction\n",
    "        outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # there are num_inputs outputs but\n",
    "        # we only want the last output\n",
    "        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "    \"\"\" MAIN PROGRAM \"\"\"\n",
    "    # Parameters\n",
    "    learning_rate = 0.0001\n",
    "    training_iters = 5000\n",
    "    display_step = 1000\n",
    "    n_input = 5\n",
    "    n_predictions = 32\n",
    "    n_hidden = 512\n",
    "    \n",
    "    # TODO Consume and concatenate multiple files (see chopin.py in repo)\n",
    "    training_data = read_data(fs_handle, input_file)\n",
    "    hdfs.log(\"training_data: {}\".format(training_data))\n",
    "    \n",
    "    # Flatten into 1D-array (needed with multiple file inputs)\n",
    "    training_data = np.concatenate(training_data).ravel()\n",
    "    \n",
    "    # Prepare dictionary/vocabulary\n",
    "    dictionary, reverse_dictionary = build_dicts(training_data)\n",
    "    vocab_size = len(dictionary)\n",
    "\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "    y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, vocab_size], mean=0.0, stddev=0.08))\n",
    "    }\n",
    "    # TODO Initialize LSTM forget gates with higher biases to encourage remembering in beginning\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([vocab_size], mean=0.0, stddev=0.8))\n",
    "    }\n",
    "\n",
    "    pred = create_rnn(x, weights, biases, n_input, n_hidden)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    with tf.name_scope('train'):\n",
    "        with tf.name_scope('adam_optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            train_op = optimizer.minimize(cost, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    # Model evaluation\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Summaries\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    logdir = tensorboard.logdir()\n",
    "    writer = tf.summary.FileWriter(logdir=logdir)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        step = 0\n",
    "        offset = random.randint(0, n_input + 1)\n",
    "        end_offset = n_input + 1\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        writer.add_graph(session.graph)\n",
    "\n",
    "        while step < training_iters:\n",
    "            if offset > (len(training_data) - end_offset):\n",
    "                # If we've stepped past our input data, restart at random offset\n",
    "                offset = random.randint(0, n_input + 1)\n",
    "\n",
    "            symbols_in_keys = [[dictionary[str(training_data[i])]] for i in range(offset, offset + n_input)]\n",
    "            symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "            symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "            symbols_out_onehot[dictionary[str(training_data[offset + n_input])]] = 1.0\n",
    "            symbols_out_onehot = np.reshape(symbols_out_onehot, [1, -1])\n",
    "\n",
    "            summary, _, acc, loss, onehot_pred = session.run(\n",
    "                [summary_op, train_op, accuracy, cost, pred],\n",
    "                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "            \n",
    "            writer.add_summary(summary, step)\n",
    "            \n",
    "            loss_total += loss\n",
    "            acc_total += acc\n",
    "            if (step + 1) % display_step == 0:\n",
    "                print(\"Iter= \" + str(step + 1) + \", Average Loss= \" +\n",
    "                      \"{:.6f}\".format(loss_total / display_step) + \", Average Accuracy= \" +\n",
    "                      \"{:.2f}%\".format(100 * acc_total / display_step))\n",
    "                acc_total = 0\n",
    "                loss_total = 0\n",
    "                symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "                symbols_out = training_data[offset + n_input]\n",
    "                symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                print(\"%s - [%s] vs [%s]\" % (symbols_in, symbols_out, symbols_out_pred))\n",
    "            step += 1\n",
    "            offset += (n_input + 1)\n",
    "        print(\"Optimization Finished!\")\n",
    "        save_path = saver.save(session, save_loc)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    # Cleanup\n",
    "    fs_handle.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Launcher\n",
    "from hops import tflauncher\n",
    "from hops import tensorboard\n",
    "\n",
    "tf_hdfs_dir = tflauncher.launch(spark, wrapper)\n",
    "tensorboard.visualize(spark, tf_hdfs_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
