JOHAN MICKOS

DEC28
    Done
    * Debugged use of global variable before use when running TFoS.
        Cause:  incorrect Python version when installing Conda (need 2.7, 3.6 was used)
        Fix:    create a new project on Hops, as that's currently the only way to change Python versions
        Note:   Hops cluster seems fully utilized and unable to allocated PySpark resources to run mini script.
                Had to wait to test change
    * Migrated chopin.py to Hops-friendly version, with basic tensorboard tracking of cost and accuracy.
      Source code has been included under the hops_notebooks directory, and can also be found on hops under the most recent
      project named "lstm_music_generator."
        Notes:  - Accuracy drops to 0 for certain iterations, we should look into why this is
                - Right now it ingests a single file under the ParsedMIDI/Sample resource directory on hops. We need to
                  add support for parsing through multiple files in a folder and concatenating their data,
                  OR modify it to assign one file per "worker" in the directory. More investigation needed here
                - It takes a long time compared to localhost when running on a single node with 1GB RAM. We need to
                  experiment with more RAM and figure out how to parallelize larger workloads.


DEC19 07:49 UTC-5
    Notes
    * There is a bug in how the LSTM code 'parses' the input and how the input is actually structured. Presently, the actual input is a series of space-separated notes/chords, implying that three non-separated notes (chord) will be treated as a 'word' by the network. We actually want the network to recognize _each_ of these notes indiviually as well, but somehow capture the fact that they occur at the same time.
        * This is an issue because certain notes are only present in chords, meaning that the network can't extrapolate information about them individually.


DEC18 09:48 UTC-5
    Done
    * Read documentation and examples on TFOS, distributed TF, and related concepts
    * Began migration of lstm.py into TFOS-compatible version. Skeleton is in place, but lacking data ingestion
    * Wrote up documentation on learned concepts, gotchas
    * Documented useful resources for further work

    Notes
    Message to Nikita:
    "So far I've made some headway re: conversion to TFOS, but the most critical outstanding part is now ingesting data into the program and converting it as the local impl. does. Included in this is looking into how information and jobs are distributed in both sycnh and asynch training on TFOS, as well as figuring out how it combines weights across machines in both cases"

    TODOS
    Below are taken from tfos.md:
    * Ingest data and populate related structures (dict, reverse_dict, training_data, ...) correctly
    * Look into asynch. vs synch. training for distributed TF and which case is applicable for us. [This](https://stackoverflow.com/questions/41293576/distributed-tensorflow-good-example-for-synchronous-training-on-cpus) may be a good resource.
    * Determine HDFS locations of textual MIDI data (may need to upload to Hops first)
    * Store trained model(s) so that we can then run prediction scripts against them to generate "music"


DEC17 08:00 UTC-5
    Done
    * Set up Nik's repo locally, ensured smooth operation
    * Read through sample code & accompanying blog bost
    * Wrote document w/ considerations and implications w.r.t. our project
    * Began investigating TensorFlowOnSpark (TFOS) requirements to get our LSTM up and running

    TODOs
    * Check shared project on Hops
        * Cannot run simple TF job using Hops' TFLauncher
            * Most likely a Python2.7 vs Python3 issue, need to verify what we have installed
    * Document and standardize output format(s) from MIDI parser/transformer
    * Write decoder for output from LSTM back into basic MIDI
        ex:
            C 7 7 7 7 ; ; ; ; A A A A 9 9 9 9 < < < < @ @ @ @ 7 7 7 7 ; ; ; ; A A A
            @ @ 7 C 7 C 7 H H H 9 ; ; ; B B B B E J J J < < < < @ @ @ @ 7 7 7 7 ; ;
